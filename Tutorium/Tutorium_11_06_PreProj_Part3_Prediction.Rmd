---
title: "Tutorium-11-06-PreProj-Part3-Prediction"
author: "dzt"
date: "6/10/2021"
output:
  html_document: default
  pdf_document: default
---
 
So far, the basic structure of prediction for project completes.
Of course, there are many other unshown ways to improve the outcome in details, that is the point to use your own fantasie to discover it. 
Note,individual's outcome should be different.



New Thema:

-Train, Test division : sample_slice(), anti_join() \n
-Prediction: predict() \n
-Hand over



 


#  1 Daten import

```{r}
library(readr)
research <- read_csv("https://raw.githubusercontent.com/sebastiansauer/willingness-vaccination-covid19/main/data/d3.csv")

```

# 2 Dataset division in Train and Test
`

Train: where create the model
Test: where test the model
```{r}
library(dplyr)
train <-  slice_sample(research, prop=.75)
train
```
```{r}
test <- anti_join(research, train, by="timestamp")
test
```

# 3 Check the missing number
```{r}
library(skimr) 
skim(train) 
```

2 age missing

```{r}
library(dplyr)
train <- select(train, -extra,-agree,-neuro,-cons,-open)
train
```

# 4 Drop cases with missing values in age


```{r}
library(tidyr)
train <-
train %>%drop_na(age) 

skim(train)
```

```{r}
train
```

#  5 Distribution of output "willingess" variablen
```{r}
library(ggplot2)
train%>%
ggplot(aes(x=willingness)) + geom_histogram()
```

#  6 Reshaping the data(pivot_longer)


```{r}

library(tidyverse)
train_pivot <-
select_if(train,is.numeric) %>%  # select only numbers
  pivot_longer(everything(), names_to = "variable")
  # everything() is a convenient shorthand to pivot all variables. pivot_longer(everything()) https://jhudatascience.org/tidyversecourse/wrangle-data.html
train
```


#  7 Distribution of all predictors 

```{r}

  ggplot(train_pivot,aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ variable, scales = "free")
```
For both the cases variables, the right-skew makes the lower end of the x-axis hard. 
To make patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics, then need to be transformed


# 8 How to transform data to normal distribution in R?  

<https://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/>
<https://moderndive.com/11-thinking-with-data.html#house-prices-EDA-I> 11.3

first try log transformation in a situation where the dependent variable starts to increase more rapidly with increasing independent variable values

log10(x) for positively skewed data,
log10(max(x+1) - x) for negatively skewed data


#  9 Transform  (via logarithm)

```{r}

train<- mutate(train,cases_log10=log10(age)) 
train
```


```{r}
select_if(train,is.numeric) %>%  # select only numbers
  pivot_longer(everything(), names_to = "variable") %>%
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ variable, scales = "free")#Each axis has a scale
```

  
# 10  Check outliers
```{r}
select_if(train,is.numeric) %>%  # select only numbers
  pivot_longer(everything(), names_to = "variable") %>%
  ggplot(aes(x = value)) +
  geom_boxplot() +
  facet_wrap(~ variable, scales = "free")
```
  

# 11 Find correlation (cormatrix)
```{r}
library(corrplot)
train <- select_if(train, is.numeric)
corrplot(cor(train), method = "circle")
```
cases_log10 and age have a strong positive correlation




Creat models, willingness as outcome, to see which variables can be used in model


# 12 Model 1 (step by step)
```{r}
model1 <- lm(willingness ~ fear, data = train)
```



# 13 Model 2
```{r}
model2 <- lm(willingness ~ fear+cons1, data = train)
```


# 14 Model 3
```{r}
model3 <- lm(willingness ~ fear + cons1 + cons2, data = train)
```

# 15 The regsubsets() function  (best sample)
(part of the leaps library) performs best subregsubsets()
set selection by identifying the best model that contains a given number
of predictors, where best is quantified using RSS
From: ISLR Book 6.5
```{r}
library (leaps)
modelfull <- regsubsets (willingness ~.,train)
summary (modelfull)
```
An asterisk indicates that a given variable is included in the corresponding
model. For instance, this output indicates that the best two-variable model
contains only fear and cons1. By default, regsubsets() only reports results
up to the best eight-variable model

# 16 The regsubsets() function,examine R2, RSS, adjusted R2, Cp, and BIC
```{r}
reg.summary =summary (modelfull)
names(reg.summary)
```
# 17 Check adjust r squared
```{r}
reg.summary$adjr2
```
For instance, we see that the R2 statistic increases from 14.2%, when only
one variable is included in the model, to almost 32.4%, when all variables
are included. As expected, the R2 statistic increases monotonically as more
variables are included.


```{r}


plot(reg.summary$adjr2 ,xlab =" Number of Variables ",
ylab=" Adjusted RSq",type="l")
```
`

# 18 Confirm model 
extra1 extra2
```{r}
model4 <- lm(willingness ~fear + extra1+ agree1+ cons1 +open1 +extra2  +neuro2 + open2  ,data = train )
```

```{r}
summary(model4)
```
# 19 Confirm model with interaktion
```{r}
model5 <- lm(willingness ~fear + extra1+ agree1+ cons1 +open1 +extra2  +neuro2 + open2 + fear:cons1+ fear:extra2  ,data = train )
```

```{r}
summary(model5)
```
# 20 Prediction

```{r}
pred <- predict(model5, newdata = test)
pred
```


```{r}
test2 <- mutate(test, pred)
test2
```


# 21 Hand over the result
```{r}
handover<- select(test2, pred,timestamp)
handover
```


```{r}
write_csv(handover, file = "student name_.csv")
```

student_name.csv is in the same folder with this rmd.datei.






Reference: 

<https://www.youtube.com/watch?v=sdfTaIC8dyE&t=257s>
<https://data-se.netlify.app/2020/11/13/fallstudie-zur-regressionsanalyse-ggplot2movies/#korrelation-zwischen-den-gruppen>

